\documentclass[10pt]{article}
\usepackage{amsgen,amsmath,amstext,amsbsy,amsopn,amssymb,mathabx,amsthm,bm,bbm,romannum,enumitem}
\usepackage[dvips]{graphicx}

\usepackage[pagebackref,bookmarksnumbered]{hyperref}
\usepackage{url}
\hypersetup{
	colorlinks=true,
	linkcolor=red,
	urlcolor=blue,
}

%\setcounter{tocdepth}{3}
%\usepackage[depth=3]{bookmark}

\usepackage[margin=1in]{geometry}
\renewcommand{\baselinestretch}{1.5}	% Line Stretch

\usepackage[utf8]{inputenc}

%----- theorems -----%

\newtheorem{thm}{Theorem}[subsection]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{coro}[thm]{Corollary}
\newtheorem{fact}[thm]{Fact}
\newtheorem*{claim}{Claim}

\theoremstyle{definition}
\newtheorem{eg}{Example}
\newtheorem*{hint}{Hint}
\newtheorem*{dfn}{Definition}
\newtheorem*{pchln}{Punchline}

\theoremstyle{remark}
\newtheorem*{rmk}{\underline{Remark}}


%----- Bolded fonts -----%

\newcommand{\ab}{\mathbf{a}}
\newcommand{\bbb}{\mathbf{b}}
\newcommand{\cbb}{\mathbf{c}}
\newcommand{\db}{\mathbf{d}}
\newcommand{\eb}{\mathbf{e}}
\newcommand{\fb}{\mathbf{f}}
\newcommand{\gb}{\mathbf{g}}
\newcommand{\hb}{\mathbf{h}}
\newcommand{\ib}{\mathbf{i}}
\newcommand{\jb}{\mathbf{j}}
\newcommand{\kb}{\mathbf{k}}
\newcommand{\lb}{\mathbf{l}}
\newcommand{\mb}{\mathbf{m}}
\newcommand{\nbb}{\mathbf{n}}
\newcommand{\ob}{\mathbf{o}}
\newcommand{\pb}{\mathbf{p}}
\newcommand{\qb}{\mathbf{q}}
\newcommand{\rb}{\mathbf{r}}
\newcommand{\sbb}{\mathbf{s}}
\newcommand{\tb}{\mathbf{t}}
\newcommand{\ub}{\mathbf{u}}
\newcommand{\vb}{\mathbf{v}}
\newcommand{\wb}{\mathbf{w}}
\newcommand{\xb}{\mathbf{x}}
\newcommand{\yb}{\mathbf{y}}
\newcommand{\zb}{\mathbf{z}}

% denote vectors
\newcommand{\ba}{\bm{a}}
\newcommand{\bb}{\bm{b}}
\newcommand{\bc}{\bm{c}}
\newcommand{\bd}{\bm{d}}
\newcommand{\be}{\bm{e}}
\newcommand{\bbf}{\bm{f}}
\newcommand{\bg}{\bm{g}}
\newcommand{\bh}{\bm{h}}
\newcommand{\bi}{\bmf{i}}
\newcommand{\bj}{\bm{j}}
\newcommand{\bk}{\bm{k}}
\newcommand{\bl}{\bm{l}}
\newcommand{\bbm}{\bm{m}}
\newcommand{\bn}{\bm{n}}
\newcommand{\bo}{\bm{o}}
\newcommand{\bp}{\bm{p}}
\newcommand{\bq}{\bm{q}}
\newcommand{\br}{\bm{r}}
\newcommand{\bs}{\bm{s}}
\newcommand{\bt}{\bm{t}}
\newcommand{\bu}{\bm{u}}
\newcommand{\bv}{\bm{v}}
\newcommand{\bw}{\bm{w}}
\newcommand{\bx}{\bm{x}}
\newcommand{\by}{\bm{y}}
\newcommand{\bz}{\bm{z}}

\newcommand{\Ab}{\mathbf{A}}
\newcommand{\Bb}{\mathbf{B}}
\newcommand{\Cb}{\mathbf{C}}
\newcommand{\Db}{\mathbf{D}}
\newcommand{\Eb}{\mathbf{E}}
\newcommand{\Fb}{\mathbf{F}}
\newcommand{\Gb}{\mathbf{G}}
\newcommand{\Hb}{\mathbf{H}}
\newcommand{\Ib}{\mathbf{I}}
\newcommand{\Jb}{\mathbf{J}}
\newcommand{\Kb}{\mathbf{K}}
\newcommand{\Lb}{\mathbf{L}}
\newcommand{\Mb}{\mathbf{M}}
\newcommand{\Nb}{\mathbf{N}}
\newcommand{\Ob}{\mathbf{O}}
\newcommand{\Pb}{\mathbf{P}}
\newcommand{\Qb}{\mathbf{Q}}
\newcommand{\Rb}{\mathbf{R}}
\newcommand{\Sbb}{\mathbf{S}}
\newcommand{\Tb}{\mathbf{T}}
\newcommand{\Ub}{\mathbf{U}}
\newcommand{\Vb}{\mathbf{V}}
\newcommand{\Wb}{\mathbf{W}}
\newcommand{\Xb}{\mathbf{X}}
\newcommand{\Yb}{\mathbf{Y}}
\newcommand{\Zb}{\mathbf{Z}}

\newcommand{\bA}{\bm{A}}
\newcommand{\bB}{\bm{B}}
\newcommand{\bC}{\bm{C}}
\newcommand{\bD}{\bm{D}}
\newcommand{\bE}{\bm{E}}
\newcommand{\bF}{\bm{F}}
\newcommand{\bG}{\bm{G}}
\newcommand{\bH}{\bm{H}}
\newcommand{\bI}{\bm{I}}
\newcommand{\bJ}{\bm{J}}
\newcommand{\bK}{\bm{K}}
\newcommand{\bL}{\bm{L}}
\newcommand{\bM}{\bm{M}}
\newcommand{\bN}{\bm{N}}
\newcommand{\bO}{\bm{O}}
\newcommand{\bP}{\bm{P}}
\newcommand{\bQ}{\bm{Q}}
\newcommand{\bR}{\bm{R}}
\newcommand{\bS}{\bm{S}}
\newcommand{\bT}{\bm{T}}
\newcommand{\bU}{\bm{U}}
\newcommand{\bV}{\bm{V}}
\newcommand{\bW}{\bm{W}}
\newcommand{\bX}{\bm{X}}
\newcommand{\bY}{\bm{Y}}
\newcommand{\bZ}{\bm{Z}}

\newcommand{\bbeta}{\bm{\beta}}
\newcommand{\balpha}{\bm{\alpha}}
\newcommand{\bgamma}{\bm{\gamma}}
\newcommand{\blambda}{\bm{\lambda}}
\newcommand{\bomega}{\bm{\omega}}
\newcommand{\bmu}{\bm{\mu}}
\newcommand{\bepsilon}{\bm{\epsilon}}
\newcommand{\btheta}{\bm{\theta}}
\newcommand{\bxi}{\bm{\xi}}
\newcommand{\bpi}{\bm{\pi}}
\newcommand{\bphi}{\bm{\phi}}

\newcommand{\bGamma}{\bm{\Gamma}}
\newcommand{\bPhi}{\bm{\Phi}}
\newcommand{\bLambda}{\bm{\Lambda}}
\newcommand{\bSigma}{\bm{\Sigma}}


%----- Blackboard bolded fonts -----%

\newcommand{\bbA}{\mathbb{A}}
\newcommand{\bbB}{\mathbb{B}}
\newcommand{\bbC}{\mathbb{C}}
\newcommand{\bbD}{\mathbb{D}}
\newcommand{\bbE}{\mathbb{E}}
\newcommand{\bbF}{\mathbb{F}}
\newcommand{\bbG}{\mathbb{G}}
\newcommand{\bbH}{\mathbb{H}}
\newcommand{\bbI}{\mathbb{I}}
\newcommand{\bbJ}{\mathbb{J}}
\newcommand{\bbK}{\mathbb{K}}
\newcommand{\bbL}{\mathbb{L}}
\newcommand{\bbM}{\mathbb{M}}
\newcommand{\bbN}{\mathbb{N}}
\newcommand{\bbO}{\mathbb{O}}
\newcommand{\bbP}{\mathbb{P}}
\newcommand{\bbQ}{\mathbb{Q}}
\newcommand{\bbR}{\mathbb{R}}
\newcommand{\bbS}{\mathbb{S}}
\newcommand{\bbT}{\mathbb{T}}
\newcommand{\bbU}{\mathbb{U}}
\newcommand{\bbV}{\mathbb{V}}
\newcommand{\bbW}{\mathbb{W}}
\newcommand{\bbX}{\mathbb{X}}
\newcommand{\bbY}{\mathbb{Y}}
\newcommand{\bbZ}{\mathbb{Z}}


%----- Calligraphic fonts -----%

\newcommand{\cA}{\mathcal{A}}
\newcommand{\cB}{\mathcal{B}}
\newcommand{\cC}{\mathcal{C}}
\newcommand{\cD}{\mathcal{D}}
\newcommand{\cE}{\mathcal{E}}
\newcommand{\cF}{\mathcal{F}}
\newcommand{\cG}{\mathcal{G}}
\newcommand{\cH}{\mathcal{H}}
\newcommand{\cI}{\mathcal{I}}
\newcommand{\cJ}{\mathcal{J}}
\newcommand{\cK}{\mathcal{K}}
\newcommand{\cL}{\mathcal{L}}
\newcommand{\cM}{\mathcal{M}}
\newcommand{\cN}{\mathcal{N}}
\newcommand{\cO}{\mathcal{O}}
\newcommand{\cP}{\mathcal{P}}
\newcommand{\cQ}{\mathcal{Q}}
\newcommand{\cR}{\mathcal{R}}
\newcommand{\cS}{\mathcal{S}}
\newcommand{\cT}{\mathcal{T}}
\newcommand{\cU}{\mathcal{U}}
\newcommand{\cV}{\mathcal{V}}
\newcommand{\cW}{\mathcal{W}}
\newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}}
\newcommand{\cZ}{\mathcal{Z}}

\newcommand\smallcO{
	\mathchoice
	{{\scriptstyle\mathcal{O}}}% \displaystyle
	{{\scriptstyle\mathcal{O}}}% \textstyle
	{{\scriptscriptstyle\mathcal{O}}}% \scriptstyle
	{\scalebox{.5}{$\scriptscriptstyle\mathcal{O}$}}%\scriptscriptstyle
}

%----- Script fonts -----%

\newcommand{\sA}{\mathscr{A}}
\newcommand{\sB}{\mathscr{B}}
\newcommand{\sC}{\mathscr{C}}
\newcommand{\sD}{\mathscr{D}}
\newcommand{\sE}{\mathscr{E}}
\newcommand{\sF}{\mathscr{F}}
\newcommand{\sG}{\mathscr{G}}
\newcommand{\sH}{\mathscr{H}}
\newcommand{\sI}{\mathscr{I}}
\newcommand{\sJ}{\mathscr{J}}
\newcommand{\sK}{\mathscr{K}}
\newcommand{\sL}{\mathscr{L}}
\newcommand{\sM}{\mathscr{M}}
\newcommand{\sN}{\mathscr{N}}
\newcommand{\sO}{\mathscr{O}}
\newcommand{\sP}{\mathscr{P}}
\newcommand{\sQ}{\mathscr{Q}}
\newcommand{\sR}{\mathscr{R}}
\newcommand{\sS}{\mathscr{S}}
\newcommand{\sT}{\mathscr{T}}
\newcommand{\sU}{\mathscr{U}}
\newcommand{\sV}{\mathscr{V}}
\newcommand{\sW}{\mathscr{W}}
\newcommand{\sX}{\mathscr{X}}
\newcommand{\sY}{\mathscr{Y}}
\newcommand{\sZ}{\mathscr{Z}}

%----- Fraktur fonts -----%

\newcommand{\fA}{\mathfrak{A}}
\newcommand{\fB}{\mathfrak{B}}
\newcommand{\fC}{\mathfrak{C}}
\newcommand{\fD}{\mathfrak{D}}
\newcommand{\fE}{\mathfrak{E}}
\newcommand{\fF}{\mathfrak{F}}
\newcommand{\fG}{\mathfrak{G}}
\newcommand{\fH}{\mathfrak{H}}
\newcommand{\fI}{\mathfrak{I}}
\newcommand{\fJ}{\mathfrak{J}}
\newcommand{\fK}{\mathfrak{K}}
\newcommand{\fL}{\mathfrak{L}}
\newcommand{\fM}{\mathfrak{M}}
\newcommand{\fN}{\mathfrak{N}}
\newcommand{\fO}{\mathfrak{O}}
\newcommand{\fP}{\mathfrak{P}}
\newcommand{\fQ}{\mathfrak{Q}}
\newcommand{\fR}{\mathfrak{R}}
\newcommand{\fS}{\mathfrak{S}}
\newcommand{\fT}{\mathfrak{T}}
\newcommand{\fU}{\mathfrak{U}}
\newcommand{\fV}{\mathfrak{V}}
\newcommand{\fW}{\mathfrak{W}}
\newcommand{\fX}{\mathfrak{X}}
\newcommand{\fY}{\mathfrak{Y}}
\newcommand{\fZ}{\mathfrak{Z}}

\newcommand{\fa}{\mathfrak{a}}
\newcommand{\ffb}{\mathfrak{b}}
\newcommand{\fc}{\mathfrak{c}}
\newcommand{\fd}{\mathfrak{d}}
\newcommand{\fe}{\mathfrak{e}}
\newcommand{\ff}{\mathfrak{f}}
\newcommand{\fg}{\mathfrak{g}}
\newcommand{\fh}{\mathfrak{h}}
\newcommand{\ffi}{\mathfrak{i}}
\newcommand{\fj}{\mathfrak{j}}
\newcommand{\fk}{\mathfrak{k}}
\newcommand{\fl}{\mathfrak{l}}
\newcommand{\fm}{\mathfrak{m}}
\newcommand{\fn}{\mathfrak{n}}
\newcommand{\fo}{\mathfrak{o}}
\newcommand{\fp}{\mathfrak{p}}
\newcommand{\fq}{\mathfrak{q}}
\newcommand{\fr}{\mathfrak{r}}
\newcommand{\fs}{\mathfrak{s}}
\newcommand{\ft}{\mathfrak{t}}
\newcommand{\fu}{\mathfrak{u}}
\newcommand{\fv}{\mathfrak{v}}
\newcommand{\fw}{\mathfrak{w}}
\newcommand{\fx}{\mathfrak{x}}
\newcommand{\fy}{\mathfrak{y}}
\newcommand{\fz}{\mathfrak{z}}

%----- special operators -----%

\newcommand{\bVar}{\mathbf{Var}}	% variance
\newcommand{\bCov}{\mathbf{Cov}}	% covariance
\newcommand{\bzero}{{\mathbf{0}}}	% zero vector
\newcommand{\bone}{{\mathbf{1}}}	% all-one vector

\newcommand{\argmin}{\mathop{\mathrm{argmin}}}
\newcommand{\argmax}{\mathop{\mathrm{argmax}}}
\newcommand{\rank}{\mathrm{rank}}	% rank
\newcommand{\sign}{\mathrm{sign}}	% sign
\newcommand{\diag}{\mathrm{diag}}	% diagonal
\newcommand{\dom}{\mathrm{dom}}		% domain
\newcommand{\ext}{\mathrm{ext}}		% Extreme
\newcommand{\diam}{\mathrm{diam}}	% diameter
\newcommand{\rdim}{\mathrm{dim}}	% dimension
\newcommand{\rker}{\mathrm{ker}}	% kernel
\newcommand{\tr}{\mathrm{tr}}	    % trace
\newcommand{\rspan}{\mathrm{span}}	% linear span
\newcommand{\supp}{\mathrm{supp}}	% support
\newcommand{\epi}{\mathrm{epi}}		% epigraph
\newcommand{\loc}{\mathrm{loc}}		% local
\newcommand{\ucp}{\mathrm{ucp}}		% uniform convergence on compacts
\newcommand{\rL}{\mathrm{L}}		% Levy Metric
\newcommand{\Wass}{\mathrm{Wass}}	% Wasserstein Metric
\newcommand{\KS}{\mathrm{KS}}		% Kolomogov-Smirnov Distance
\newcommand{\LP}{\mathrm{LP}}		% Levy-Prokhorov Metric
\newcommand{\TV}{\mathrm{TV}}		% Total Variation
\newcommand{\Lips}{\mathrm{Lips}}	% Lipschitz Metric
\newcommand{\rS}{\mathrm{S}}		% Skorokhod Topology
\newcommand{\rd}{\mathrm{d}}		% differentiation
\newcommand{\rF}{\mathrm{F}}		% Frobenius
\newcommand{\coup}{\mathrm{coup}}	% coupling
\newcommand{\mix}{\mathrm{mix}}		% mixing
\newcommand{\rel}{\mathrm{rel}}		% relaxation
\newcommand{\set}{\mathrm{set}}		% set
\newcommand{\LOOCV}{\mathrm{LOOCV}}	% leave-one-out cross-validation

\newcommand{\bbone}{{\mathbbm{1}}}	% indicator

\newcommand\munderbar[1]{%
	\underaccent{\bar}{#1}}

\newcommand\indep{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}	% independence

%----- distribution name -----%

\newcommand{\Exp}{\textbf{Exponential}}
\newcommand{\Pois}{\textbf{Poisson}}
\newcommand{\Gumb}{\textbf{Gumbel}}
\newcommand{\Bern}{\textbf{Bernoulli}}
\newcommand{\Bin}{\textbf{Binomial}}
\newcommand{\NBin}{\textbf{Negative-Binomial}}
\newcommand{\Multi}{\textbf{Multinomial}}
\newcommand{\Geo}{\textbf{Geometric}}
\newcommand{\Hyper}{\textbf{Hypergeometric}}
\newcommand{\Cauchy}{\textbf{Cauchy}}
\newcommand{\Levy}{\textbf{L\'{e}vy}}
\newcommand{\Unif}{\textbf{Uniform}}

\usepackage{titling}

% Create subtitle command for use in maketitle
\newcommand{\subtitle}[1]{
	\posttitle{
		\begin{center}\large#1\end{center}
	}
}

\setlength{\droptitle}{-2em}
\title{STOR 767 Spring 2019 Hw4}
\pretitle{\vspace{\droptitle}\centering\huge}
\posttitle{\par}
\subtitle{\textbf{Due on 02/27/2019 in Class}}
\author{\it Zhenghan Fang}
%\preauthor{}\postauthor{}
\date{}
\predate{}\postdate{}

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\rhead{Zhenghan Fang}
\cfoot{\thepage}
\renewcommand{\headrulewidth}{0pt}

\begin{document}
	\pagenumbering{arabic}
	\maketitle
	
	% \pdfbookmark[<level>]{<title>}{<dest>}
	%\pdfbookmark[section]{\contentsname}{toc}

\begin{rmk}
	This homework focuses on splines and kernel methods. 
\end{rmk}

\noindent \textbf{Instruction.}
\begin{itemize}
	\item {\bf Theoretical Part and Computational Part are respectively credited 60 points. At most 100 points in total will be accounted for this homework.}
	
	\item Submission of handwritten solution for the \textbf{Theoretical Part} of this homework is allowed.
	
	\item Please use \textbf{RMarkdown} to create a formatted report for the \textbf{Computational Part} of this homework. 

	\item Some of the problems are selected or modified from the textbook \cite{friedman2009elements}.
	
\end{itemize}

\section*{Theoretical Part}
\begin{enumerate}
	\item (\textit{15 pt}) (Textbook Ex. 5.1) Consider the following truncated power basis with 2 knots $ \xi_{1},\xi_{2} \in \bbR $
	\[ h_{1}(x) = 1, \quad h_{2}(x) = x, \quad h_{3}(x) = x^{2}, \quad h_{4}(x) = x^{3}, \quad h_{5}(x) = (x - \xi_{1})_{+}^{3}, \quad h_{6}(x) = (x - \xi_{2})_{+}^{3}. \quad(x \in \bbR) \]
	Show that it represents a basis for a cubic spline with knots $\xi_{1},\xi_{2}$. 
	
	Let $f(x) = \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3 + \theta_1 (x-\xi_1)_+^3 + \theta_2 (x-\xi_2)_+^3$. Then,
	\begin{gather*}
	    f'(x) = \beta_1 + 2\beta_2 x + 3\beta_3 x^2 + 3\theta_1 (x-\xi_1)_+^2 + 3\theta_2 (x-\xi_2)_+^2 \\
	    f''(x) = 2\beta_2 + 6\beta_3 x + 6\theta_1 (x-\xi_1)_+ + 6\theta_2 (x-\xi_2)_+
	\end{gather*}
	Then,
	\begin{gather*}
	    f(\xi_1^-) = f(\xi_1^+)  \\
	    f'(\xi_1^-) = f'(\xi_1^+)  \\
	    f''(\xi_1^-) = f''(\xi_1^+)  \\
	    f(\xi_2^-) = f(\xi_2^+)  \\
	    f'(\xi_2^-) = f'(\xi_2^+)  \\
	    f''(\xi_2^-) = f''(\xi_2^+) 
	\end{gather*}
	Therefore, $\{h_1(x), ... ,h_6(x)\}$ represents a basis for a cubic spline.

	\item (\textit{15 pt}) (Textbook Ex. 5.4) Consider the truncated power series representation for cubic splines with $ K $-knots $ \{ \xi_{k} \}_{k=1}^{K} \subseteq \bbR $
	\[ f(x) = \sum_{j=0}^{3}\beta_{j}x^{j} + \sum_{k=1}^{K}\theta_{k}(x - \xi_{k})_{+}^{3}. \quad(x \in \bbR) \]
	\begin{itemize}
		\item [(a)] Prove that the natural boundary conditions (\textit{i.e.} $ f $ is linear on $ (-\infty,\xi_{1}]\cup[\xi_{K},+\infty) $) is equivalently to the coefficient constraints
		\[ \beta_{2} = \beta_{3} = \sum_{k=1}^{K}\theta_{k} = \sum_{k=1}^{K}\xi_{k}\theta_{k} = 0. \]
		
		When $x \in (-\infty,\xi_{1}]$, \[f(x) = \beta_{0} + \beta_{1}x+ \beta_{2}x^{2} + \beta_{3}x^{3} \]
		When $x \in [\xi_{K},+\infty)$, \[f(x) = \sum_{j=0}^{3}\beta_{j}x^{j} + \sum_{k=1}^{K}\theta_{k}(x - \xi_{k})^{3} = \left(\beta_3 + \sum_{k=1}^{K}\theta_k\right)x^3 - 3\left(\beta_2 + \sum_{k=1}^{K}\xi_{k}\theta_{k}\right)x^2 + ax+b\]
		Therefore, the natural boundary conditions is equivalent to
		\[ \beta_{2} = \beta_{3} = \sum_{k=1}^{K}\theta_{k} = \sum_{k=1}^{K}\xi_{k}\theta_{k} = 0. \]
		
		\item [(b)] Derive the natural cubic splines. That is, argue the set of basis functions (linearly independent, and in a special for of truncated basis)
		\[ N_{1}(x) = 1, \quad N_{2}(x) = x, \quad N_{k+2}(x) = d_{k}(x) - d_{K-1}(x) \quad(x \in \bbR,\ 1 \le k \le K-2) \]
		where
		\[ d_{k}(x) = {(x - \xi_{k})_{+}^{3} - (x - \xi_{K})^{3}_{+} \over \xi_{K} - \xi_{k}} \quad(x \in \bbR,\ 1 \le k \le K-2) \]
		can represent any truncated power basis under the constraints in (a).
		
		For any $\alpha d_k(x)$, $\alpha \in \bbR$, $1\le k \le K-1$, \[\sum_{k=1}^K \theta_k = \frac{\alpha}{\xi_K-\xi_k} - \frac{\alpha}{\xi_K-\xi_k} = 0\]
		
		For any $\alpha N_{k+2}(x)$, $\alpha \in \bbR$, $1\le k \le K-1$, \[ \sum_{k=1}^{K}\xi_{k}\theta_{k} = \frac{\alpha\xi_k}{\xi_K-\xi_k} - \frac{\alpha\xi_K}{\xi_K-\xi_k} - \frac{\alpha\xi_{K-1}}{\xi_K-\xi_{K-1}} + \frac{\alpha\xi_{K}}{\xi_K-\xi_{K-1}} =0 \]
		
		Therefore, $\{N_1(x), ..., N_K(x)\}$ can represent any truncated pawer basis under the constraints in (a).
		
	\end{itemize}

	\item (\textit{15 pt}) (Smoothing Splines, Textbook Ex. 5.7) Suppose $ N \ge 2 $, and that $ g $ is the natural cubic spline interpolant to the pairs $ \{ (x_{i},z_{i}) \}_{i=1}^{N} $, with $ -\infty < a < x_{1} < \cdots < x_{N} < b < +\infty $, \textit{i.e.} $ g $ as a cubic spline with knots $ \{ x_{i} \}_{i=1}^{N} $ satisfies
	\[ g(x_{i}) = z_{i}. \quad(1 \le i \le N) \]
	Let $ \widetilde{g} \in C^{2}[a,b] $ be any other twice continuously differentiable function supported on $ [a,b] $ that interpolates the $ N $ pairs.
	
	\begin{itemize}
		\item [(a)] Let $ r = \widetilde{g} - g $ be the residual. Show that $ g'' $ is orthogonal to $ r'' $ in $ L^{2}(\rd \fm) $ (denoting $ \fm $ as the Lebesgue measure). That is,
		\begin{align*}
		\langle g'',r'' \rangle &:= \int_{a}^{b}g''(x)r''(x)\rd x\\
		&= - \sum_{j=1}^{N}g'''(x_{j}^{+})[r(x_{j+1}) - r(x_{j})] \quad(\text{by integration-by-part})\\
		&= 0.
		\end{align*}
		\begin{hint}
			Note the stepwise constancies of $ g''' $ as a cubic spline function. Take care of the non-smoothness at knots in integration-by-part.
		\end{hint}
		
		By integration-by-part,
		\begin{align*}
		\langle g'',r'' \rangle = \int_{a}^{b}g''(x)r''(x)\rd x = g''(x)r'(x) |^b_a - \int_{a}^{b}g'''(x)r'(x)\rd x
		\end{align*}
		Because $g$ is a natural spline, $g''(a) = g''(b) = 0$. Then,
		\begin{align*}
		\langle g'',r'' \rangle & = - \int_{a}^{b}g'''(x)r'(x)\rd x \\
		& = - \sum_{j=1}^{N-1} \int_{x_j}^{x_{j+1}}g'''(x)r'(x)\rd x - \int_{a}^{x_1}g'''(x)r'(x)\rd x - \int_{x_N}^{b}g'''(x)r'(x)\rd x 
		\end{align*}
		Because $g'''$ is constant in $[x_j,x_{j+1}]$, and $g'''(x)=0$ when $x \le x_1 $ and $x \ge x_N$,
		\begin{align*}
		\langle g'',r'' \rangle & = - \sum_{j=1}^{N-1} g'''(x_j^+)\int_{x_j}^{x_{j+1}}r'(x)\rd x  \\
		& = - \sum_{j=1}^{N-1} g'''(x_j^+)[r(x_{j+1}) - r(x_j)] \\
		& = 0 \quad (r(x_j) = 0, j=1,...N)
		\end{align*}
	
		\item [(b)] Show the Pythagorean identity
		\[ \|\widetilde{g}''\|_{L^{2}(\rd \fm)}^{2} = \|g''\|_{L^{2}(\rd \fm)}^{2} + \|r''\|_{L^{2}(\rd \fm)}^{2} \ge \|g''\|_{L^{2}(\rd \fm)}^{2}, \]
		and conclude that
		\[ \int_{a}^{b}\widetilde{g}''(x)^{2}\rd x \ge \int_{a}^{b}g''(x)^{2}\rd x, \]
		with equality if and only if $ g = \widetilde{g} $ \textit{a.e.} $ \fm $\footnote{A more profound result \href{https://en.wikipedia.org/wiki/Liouville\%27s_theorem_(complex_analysis)}{Liouville's Theorem} from complex analysis, applied to PDE, dynamical system \textit{etc.}, can be stated that any harmonic function (\textit{i.e.} with second-order differential 0) being constant at boundary must remain constant over the entire domain.}.
		
		\begin{align*}
		    \|\widetilde{g}''\|_{L^{2}(\rd \fm)}^{2} & = \int_{a}^{b}\widetilde{g}''(x)^{2}\rd x \\
		    & = \int_{a}^{b}(g''(x) + r''(x))^{2}\rd x \\
		    & = \int_{a}^{b}g''(x)^{2}\rd x + \int_{a}^{b}r''(x)^{2}\rd x \quad(\int_{a}^{b}r''(x)g''(x) \rd x = 0) \\
		    & = \|g''\|_{L^{2}(\rd \fm)}^{2} + \|r''\|_{L^{2}(\rd \fm)}^{2} \ge \|g''\|_{L^{2}(\rd \fm)}^{2}
		\end{align*}
		Therefore, \[ \int_{a}^{b}\widetilde{g}''(x)^{2}\rd x \ge \int_{a}^{b}g''(x)^{2}\rd x. \]
		Derive the condition for equality. If \[ \int_{a}^{b}\widetilde{g}''(x)^{2}\rd x = \int_{a}^{b}g''(x)^{2}\rd x, \]
		then
		\begin{align*}
		    & \int_{a}^{b}r''(x)^2 \rd x = \int_{a}^{b}\widetilde{g}''(x)^{2}\rd x - \int_{a}^{b}g''(x)^{2}\rd x = 0 \\
		    \implies & r''(x) = 0 \\
		    \implies & r(x) = 0 \quad (r(x_1) = r(x_N) = 0, \text{Liouville's Theorem})
		\end{align*}
		Therefore, equality holds if and only if $ g = \widetilde{g} $.
		
		
		
		
		\item [(c)] Argue that the solution to the smoothing spline problem
		\begin{align}
		\min_{f \in C^{2}[a,b]} \sum_{i=1}^{N}[y_{i}-f(x_{i})]^{2} + \lambda \int_{a}^{b}f''(x)^{2}\rd x\label{eq_smooth_sp}
		\end{align}
		(for some $ \lambda > 0 $) must be a natural cubic spline with knots at $ \{ x_{i} \}_{i=1}^{N} $\footnote{It suggests that the smoothing spline optimization over an infinite-dimensional functional space is actually a $ N $-dimensional problem given the pairs $ \{ (x_{i},y_{i}) \}_{i=1}^{N} $. We call $ C^{2}[a,b] $ equipped with norm $ f\mapsto \|f\|_{2,2} := \|f\|_{2} + \|f''\|_{2} $ as the Sobolev space $ W^{2,2}[a,b] $.}.
		
		\begin{hint}
			The solution $ \widehat{f}_{\lambda} $ might not interpolate $ \{ (x_{i},y_{i}) \}_{i=1}^{N} $, but one might consider $ g $ interpolates $ \{ (x_{i},\widehat{f}_{\lambda}(x_{i})) \}_{i=1}^{N} $.
		\end{hint}
		
		Let $ \widehat{f}_{\lambda}(x) $ be the solution at a certain $\lambda$. Let $g(x)$ be the natural cubic spline with knots at $ \{ x_{i} \}_{i=1}^{N} $ that interpolates $ \{ (x_{i},\widehat{f}_{\lambda}(x_{i})) \}_{i=1}^{N} $. 
		
		If $\widehat{f}_{\lambda} \ne g$, then \[ \int_{a}^{b}\widehat{f}_{\lambda}''(x)^{2}\rd x > \int_{a}^{b} g''(x)^{2}\rd x, \]
		\[ \sum_{i=1}^{N}[y_{i}-\widehat{f}_{\lambda}(x_{i})]^{2} = \sum_{i=1}^{N}[y_{i}-g(x_{i})]^{2} \quad (g(x_i) = \widehat{f}_{\lambda}(x_{i})) ).\]
		Then, $\widehat{f}_{\lambda}$ has a greater objective function value than $g$, which contradicts with that $\widehat{f}_{\lambda}$ is the minimal solution. 
		
		Therefore, $\widehat{f}_{\lambda} = g$.
		
	\end{itemize}
	
	\item (\textit{15 pt}) (Leave-One-Out, Textbook Ex. 5.13) Let $ \widehat{f}_{\lambda} $ be the solution to (\ref{eq_smooth_sp}) given $ N $ pairs $ \cD := \{ (x_{i},y_{i}) \}_{i=1}^{N} $ and $ \lambda > 0 $. 
	\begin{itemize}
		\item [(a)] Suppose you augment the training sample with another pair $ \left( x_{0},\widehat{f}_{\lambda}(x_{0}) \right) $ and refit. Argue that the refitted solution remains unchanged.
		
		Let 
		\begin{align*}
		    & L_1(f) = \sum_{i=1}^{N}[y_{i}-f(x_{i})]^{2} + \lambda \int_{a}^{b}f''(x)^{2}\rd x, \\
		    & L_2(f) = [\widehat{f}_{\lambda}(x_{0})-f(x_{0})]^{2}.
		\end{align*}
		After augmentation, the new problem is 
		\[\min_{f \in C^{2}[a,b]} L_1(f) + L_2(f). \]
		Because $ \widehat{f}_{\lambda} $ minimizes $L_1(f)$ and $L_2(f)$ simultaneously, $ \widehat{f}_{\lambda} $ minimizes $L_1(f) + L_2(f)$, i.e. $\widehat{f}_{\lambda}$ is the solution for the new problem.
		
		
		
		\item [(b)] Recall that with the smoother matrix $ \Sbb_{\lambda} = [s_{\lambda,ij}]_{N\times N} $, $ \widehat{\bbf}_{\lambda} = \Sbb_{\lambda}\by $ gives the vector of in-sample predictions. Show that the individual leave-one-out error
		\begin{align}
		e_{i} := y_{i} - \widehat{f}^{(-i)}_{\lambda}(x_{i}) = {y_{i} - \widehat{f}_{\lambda}(x_{i}) \over 1 - s_{\lambda,ii}} \quad(1 \le i \le N)\label{eq_loo}
		\end{align}
		where $ \widehat{f}^{(-i)}_{\lambda} $ is prediction function based on sample $ \cD \backslash \{ (x_{i},y_{i}) \} $, hence prove the leave-one-out cross-validation (LOOCV) criteria\footnote{It generalizes the influence measures as discussed in STOR 664, where $ \Sbb_{\lambda} = \Hb = [h_{ij}]_{N\times N} $ is the hat matrix, $ h_{ii} $ is called the \textbf{leverage}, (\ref{eq_loo}) is the \textbf{DFFITS} for the \textit{i}-th observation, and its square proportionates to the \textbf{Cook's distance}.}
		\[ \LOOCV(\widehat{f}_{\lambda}) := {1 \over N}\sum_{i=1}^{N}\left( y_{i} - \widehat{f}^{(-i)}_{\lambda}(x_{i}) \right)^{2} = {1 \over N}\sum_{i=1}^{N}\left( {y_{i} - \widehat{f}_{\lambda}(x_{i}) \over 1 - s_{\lambda,ii}} \right)^{2}. \]
		
		\begin{hint}
			Establish an equation by cooking up a refitting stable situation as in (a).
		\end{hint}
		
		Let $g^{(-i)}_{\lambda}$ be the prediction function based on sample $( \cD \backslash \{ (x_{i},y_{i}) \} ) \cup \{ (x_{i},\widehat{f}^{(-i)}_{\lambda}(x_{i})) \}$. By conclusion from (a), $g^{(-i)}_{\lambda} = \widehat{f}^{(-i)}_{\lambda}$. The smoother matrix of $g^{(-i)}_{\lambda}$ is $ \Sbb_{\lambda}$. Therefore, 
		\begin{align*}
		    g^{(-i)}_{\lambda}(x_i) & = \sum_{j \ne i} s_{\lambda,ij} y_j + s_{\lambda,ii} \widehat{f}^{(-i)}_{\lambda}(x_{i}) \\
		    & = \sum_{j = 1}^{N} s_{\lambda,ij} y_j - s_{\lambda,ii} y_i + s_{\lambda,ii} \widehat{f}^{(-i)}_{\lambda}(x_{i}) \\
		    & = \widehat{f}_{\lambda}(x_{i}) - s_{\lambda,ii} y_i + s_{\lambda,ii} \widehat{f}^{(-i)}_{\lambda}(x_{i})
		\end{align*}
		Then,
		\begin{align*}
		    & \widehat{f}^{(-i)}_{\lambda}(x_i) = g^{(-i)}_{\lambda}(x_i) = \widehat{f}_{\lambda}(x_i) - s_{\lambda,ii}y_i + s_{\lambda,ii} \widehat{f}^{(-i)}_{\lambda}(x_i) \\
		    \implies & \widehat{f}^{(-i)}_{\lambda}(x_i) = \frac{\widehat{f}_{\lambda}(x_i) - s_{\lambda,ii}y_i}{1-s_{\lambda,ii}} \\
		    \implies & y_{i} - \widehat{f}^{(-i)}_{\lambda}(x_{i}) = {y_{i} - \widehat{f}_{\lambda}(x_{i}) \over 1 - s_{\lambda,ii}}
		\end{align*}
	\end{itemize}
	\end{enumerate}

\section*{Computational Part}

\begin{enumerate}
	\item (\textit{20 pt}) \textbf{Simulation} \\
	Consider the simulation setup as in Section 5.5.2
	\[ X \sim \Unif[0,1] \indep \epsilon \sim \cN(0,1), \quad f(X) := {\sin(12(X+0.2)) \over X+0.2}, \quad Y = f(X) + \epsilon \]
	with $ N = 100 $ randomly generated training sample. Fit polynomial regression, B-spline, natural cubic spline, smoothing spline and local polynomial regression with various kernels. Use cross-validation to tune any parameters. Compare their performances on a 10,000 test set.
	 
	\item (\textit{20 pt}) \textbf{Zip Code Digit Data} (Textbook Ex. 6.12) \\
	Write a computer program to perform a local linear discriminant analysis. At each query point $ x_{0} $, the training data $ \{ (x_{i},y_{i}) \}_{i=1}^{N} $ receive weights $ \{K_{\lambda}(x_{0}, x_{i})\}_{i=1}^{n} $ from a weighting kernel $ K_{\lambda} $, and return a weighted least-square discriminant prediction. Try out your program on the \textit{Zip Code Digits Data} to discriminate 3's and 8's with various kernel functions.
	
	\begin{hint}
		\textbf{R} package \texttt{kernlab} might be helpful to get various kernel functions and compute their linear form and quadratic form efficiently. Parameters indexing the kernel function should be tuned via cross-validation.
	\end{hint}
	
	\item (\textit{20 pt}) \textbf{Phoneme Recognition Data} \\
	One can use splines not only to increase flexibility of the functional modeling, but also to reduce the flexibility. In Section 5.2.3, natural cubic splines are used to simplify the input signals which have strong positive autocorrelation. Reproduce the analysis as is done in the textbook and also work on Ex. 5.5. The \textit{Phoneme Recognition Data} are available at \url{https://web.stanford.edu/~hastie/ElemStatLearn/datasets/phoneme.data}. Report both the R code and the results.
\end{enumerate}

\bibliography{bibfile.bib}
\bibliographystyle{plain}
\end{document}